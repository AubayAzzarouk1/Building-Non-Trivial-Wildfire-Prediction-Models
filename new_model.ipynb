!pip install rasterio
import os
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import rasterio
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader, random_split, Subset
from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix
from tqdm import tqdm
import warnings
import seaborn as sns
try:
    from google.colab import drive
except ImportError:
    pass  # Not running in Google Colab

# Suppress warnings
warnings.filterwarnings('ignore')

# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Constants for band indices
NDVI_IDX = (3, 0)  # NIR and Red band indices for NDVI calculation
NBR_IDX = (3, 4)   # NIR and SWIR band indices for NBR calculation

#----------------------------------------------------------------------------
# Robust Dataset Implementation
#----------------------------------------------------------------------------

class RobustWildfireDataset(Dataset):
    """
    Enhanced dataset class with robust handling of inconsistent data
    """
    def __init__(self, tif_folder, seq_length=7, input_channels=6,
                 fire_threshold=0.3, include_indices=True, target_size=(112, 112),
                 fire_band_idx=0, augment=False):
        self.tif_files = sorted([os.path.join(tif_folder, f) for f in os.listdir(tif_folder)
                                if f.endswith(".tif")])
        self.seq_length = seq_length

        # Get image dimensions and actual channel count from first file
        with rasterio.open(self.tif_files[0]) as src:
            sample = src.read()

        # Determine the actual number of channels in the input data
        self.actual_channels = sample.shape[0]
        print(f"Actual channels in the data: {self.actual_channels}")

        # Adjust input_channels to not exceed actual channels
        self.base_channels = min(input_channels, self.actual_channels)
        self.include_indices = include_indices

        # Check if we can calculate the requested indices
        self.can_calc_ndvi = all(idx < self.actual_channels for idx in NDVI_IDX)
        self.can_calc_nbr = all(idx < self.actual_channels for idx in NBR_IDX)

        # Determine how many derived indices we can actually include
        derived_indices_count = 0
        if include_indices:
            if self.can_calc_ndvi:
                derived_indices_count += 1
            if self.can_calc_nbr:
                derived_indices_count += 1

        self.input_channels = self.base_channels + derived_indices_count

        # Adjust fire band index if it's out of range
        if fire_band_idx >= self.base_channels:
            print(f"WARNING: Specified fire_band_idx {fire_band_idx} is out of range. Using band 0 instead.")
            self.fire_band_idx = 0
        else:
            self.fire_band_idx = fire_band_idx

        self.fire_threshold = fire_threshold
        self.target_size = target_size
        self.augment = augment

        self.height, self.width = sample.shape[1], sample.shape[2]

        if len(self.tif_files) < seq_length:
            raise ValueError(f"Need at least {seq_length} TIF files, but found only {len(self.tif_files)}")

        print(f"Original image dimensions: {self.height}x{self.width}, Actual channels: {self.actual_channels}")
        print(f"Using base channels: {self.base_channels}, Total channels: {self.input_channels}")
        print(f"Can calculate NDVI: {self.can_calc_ndvi}, Can calculate NBR: {self.can_calc_nbr}")
        print(f"Target image dimensions: {target_size[0]}x{target_size[1]}")
        print(f"Using band {self.fire_band_idx} for fire detection with threshold {fire_threshold}")
        print(f"Data augmentation: {augment}")

        # Verify all files have the same number of channels
        self._verify_channel_consistency()

    def _verify_channel_consistency(self):
        """Check that all files have the same number of channels"""
        try:
            # Check a subset of files (first, middle, last) to save time
            check_indices = [0, len(self.tif_files)//2, -1]
            channel_counts = []

            for idx in check_indices:
                if idx < 0:
                    idx = len(self.tif_files) + idx  # Handle negative indexing
                with rasterio.open(self.tif_files[idx]) as src:
                    channel_counts.append(src.count)

            if len(set(channel_counts)) > 1:
                print(f"WARNING: Inconsistent channel counts detected in files: {channel_counts}")
                print("The dataset will attempt to handle this automatically.")
        except Exception as e:
            print(f"Error checking channel consistency: {e}")
            print("Will proceed but may encounter errors during loading.")

    def __len__(self):
        return len(self.tif_files) - self.seq_length

    def calculate_indices(self, image):
        """Calculate spectral indices from bands with robust handling"""
        indices = []

        # Only calculate NDVI if we have the required bands
        if self.can_calc_ndvi:
            try:
                # NDVI = (NIR - Red) / (NIR + Red)
                nir, red = image[NDVI_IDX[0]], image[NDVI_IDX[1]]
                ndvi = np.zeros_like(nir)
                valid_mask = (nir + red) != 0
                ndvi[valid_mask] = (nir[valid_mask] - red[valid_mask]) / (nir[valid_mask] + red[valid_mask])
                ndvi = np.clip(ndvi, -1, 1)
                indices.append(ndvi)
            except Exception as e:
                print(f"Error calculating NDVI: {e}")
                # Add zero-filled array instead
                indices.append(np.zeros_like(image[0]))

        # Only calculate NBR if we have the required bands
        if self.can_calc_nbr:
            try:
                # NBR = (NIR - SWIR) / (NIR + SWIR)
                nir, swir = image[NBR_IDX[0]], image[NBR_IDX[1]]
                nbr = np.zeros_like(nir)
                valid_mask = (nir + swir) != 0
                nbr[valid_mask] = (nir[valid_mask] - swir[valid_mask]) / (nir[valid_mask] + swir[valid_mask])
                nbr = np.clip(nbr, -1, 1)
                indices.append(nbr)
            except Exception as e:
                print(f"Error calculating NBR: {e}")
                # Add zero-filled array instead
                indices.append(np.zeros_like(image[0]))

        # If we have indices, stack them; otherwise return empty array
        if indices:
            return np.stack(indices)
        else:
            return np.zeros((0, image.shape[1], image.shape[2]), dtype=np.float32)

    def apply_simple_augmentation(self, sequence, target):
        """Apply simple data augmentation"""
        if not self.augment:
            return sequence, target

        # 50% chance of horizontal flip
        if torch.rand(1).item() > 0.5:
            sequence = torch.flip(sequence, [3])  # Flip width dimension
            target = torch.flip(target, [2])      # Flip width dimension

        # 25% chance of vertical flip
        if torch.rand(1).item() > 0.75:
            sequence = torch.flip(sequence, [2])  # Flip height dimension
            target = torch.flip(target, [1])      # Flip height dimension

        # 25% chance of random brightness adjustment (only for visualization channels)
        if torch.rand(1).item() > 0.75:
            brightness_factor = torch.rand(1).item() * 0.4 + 0.8  # 0.8 to 1.2
            # Only apply to channels 0-2 if we have that many channels (usually RGB)
            vis_channels = min(3, sequence.shape[1])
            sequence[:, :vis_channels] = sequence[:, :vis_channels] * brightness_factor
            sequence[:, :vis_channels] = torch.clamp(sequence[:, :vis_channels], 0, 1)

        return sequence, target

    def __getitem__(self, idx):
        # Pre-allocate full sequence tensor with target dimensions
        sequence_tensor = torch.zeros(
            (self.seq_length, self.input_channels, self.target_size[0], self.target_size[1]),
            dtype=torch.float32
        )

        # Load and process each frame
        for i in range(self.seq_length):
            try:
                file_idx = idx + i
                if file_idx >= len(self.tif_files):
                    print(f"Warning: File index {file_idx} out of bounds. Using last available file.")
                    file_idx = len(self.tif_files) - 1

                with rasterio.open(self.tif_files[file_idx]) as src:
                    image = src.read().astype(np.float32)

                    # Handle case where file has unexpected number of channels
                    if image.shape[0] != self.actual_channels:

                        # Pad or truncate as needed
                        if image.shape[0] < self.base_channels:
                            # Pad with zeros
                            padding = np.zeros((self.base_channels - image.shape[0], image.shape[1], image.shape[2]), dtype=np.float32)
                            image = np.concatenate([image, padding], axis=0)
                        else:
                            # Truncate to expected channels
                            image = image[:self.base_channels]

                    # Normalize each band independently to [0,1]
                    for band in range(image.shape[0]):
                        # Skip empty bands
                        if np.all(image[band] == 0):
                            continue

                        band_min = np.nanmin(image[band])
                        band_max = np.nanmax(image[band])
                        if band_max > band_min:  # Avoid division by zero
                            image[band] = (image[band] - band_min) / (band_max - band_min)

                    # Replace NaN and infinity values
                    image = np.nan_to_num(image, nan=0.0, posinf=1.0, neginf=0.0)

                    # Ensure we only take the base channels
                    if image.shape[0] > self.base_channels:
                        base_image = image[:self.base_channels]
                    else:
                        base_image = image

                    # Calculate indices if needed
                    if self.include_indices and (self.can_calc_ndvi or self.can_calc_nbr):
                        indices = self.calculate_indices(image)
                        if indices.shape[0] > 0:  # Only concatenate if we have indices
                            full_image = np.concatenate([base_image, indices], axis=0)
                        else:
                            full_image = base_image
                    else:
                        full_image = base_image

                    # Convert to tensor
                    tensor_image = torch.from_numpy(full_image)

                    # Resize using torchF.interpolate if dimensions don't match target
                    if (tensor_image.shape[1], tensor_image.shape[2]) != self.target_size:
                        tensor_image = tensor_image.unsqueeze(0)
                        tensor_image = F.interpolate(
                            tensor_image,
                            size=self.target_size,
                            mode='bilinear',
                            align_corners=False
                        )
                        tensor_image = tensor_image.squeeze(0)

                    # Store in sequence tensor
                    sequence_tensor[i] = tensor_image

            except Exception as e:
                print(f"Error processing file {self.tif_files[idx + i]}: {e}")
                # Continue with next file - the affected frame will remain zeros

        # Get fire mask from last frame using the specified band
        fire_mask = (sequence_tensor[-1, self.fire_band_idx:self.fire_band_idx+1, :, :] > self.fire_threshold).float()

        # Apply simple augmentation if enabled
        sequence_tensor, fire_mask = self.apply_simple_augmentation(sequence_tensor, fire_mask)

        return sequence_tensor, fire_mask

#----------------------------------------------------------------------------
# Loss Functions
#----------------------------------------------------------------------------

class DiceLoss(nn.Module):
    """Dice Loss for better handling of class imbalance"""
    def __init__(self, smooth=1.0):
        super(DiceLoss, self).__init__()
        self.smooth = smooth

    def forward(self, logits, targets):
        probs = torch.sigmoid(logits)

        # Flatten
        probs_flat = probs.view(-1)
        targets_flat = targets.view(-1)

        # Intersection and union
        intersection = (probs_flat * targets_flat).sum()
        union = probs_flat.sum() + targets_flat.sum()

        # Dice coefficient
        dice = (2.0 * intersection + self.smooth) / (union + self.smooth)

        return 1.0 - dice


class FocalLoss(nn.Module):
    """Focal Loss to focus on hard examples"""
    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction

    def forward(self, logits, targets):
        probs = torch.sigmoid(logits)

        # Binary cross entropy
        bce = F.binary_cross_entropy_with_logits(logits, targets, reduction='none')

        # Focal weighting
        p_t = probs * targets + (1 - probs) * (1 - targets)
        alpha_t = self.alpha * targets + (1 - self.alpha) * (1 - targets)
        focal_weight = alpha_t * (1 - p_t).pow(self.gamma)

        loss = focal_weight * bce

        if self.reduction == 'mean':
            return loss.mean()
        elif self.reduction == 'sum':
            return loss.sum()
        else:  # 'none'
            return loss


class CombinedLoss(nn.Module):
    """Combination of BCE, Dice, and Focal losses"""
    def __init__(self, bce_weight=1.0, dice_weight=1.0, focal_weight=0.5, pos_weight=None):
        super(CombinedLoss, self).__init__()
        self.bce_weight = bce_weight
        self.dice_weight = dice_weight
        self.focal_weight = focal_weight

        self.bce = nn.BCEWithLogitsLoss(pos_weight=pos_weight) if pos_weight is not None else nn.BCEWithLogitsLoss()
        self.dice = DiceLoss()
        self.focal = FocalLoss()

    def forward(self, logits, targets):
        bce_loss = self.bce(logits, targets)
        dice_loss = self.dice(logits, targets)
        focal_loss = self.focal(logits, targets)

        combined = (self.bce_weight * bce_loss +
                    self.dice_weight * dice_loss +
                    self.focal_weight * focal_loss)

        return combined

#----------------------------------------------------------------------------
# Model Building Blocks
#----------------------------------------------------------------------------

class AttentionBlock(nn.Module):
    """Self-attention block for focusing on important areas"""
    def __init__(self, in_channels):
        super(AttentionBlock, self).__init__()
        # Ensure channel division doesn't result in zero channels
        reduction_factor = 8 if in_channels >= 8 else 2
        self.query = nn.Conv2d(in_channels, max(1, in_channels//reduction_factor), kernel_size=1)
        self.key = nn.Conv2d(in_channels, max(1, in_channels//reduction_factor), kernel_size=1)
        self.value = nn.Conv2d(in_channels, in_channels, kernel_size=1)
        self.gamma = nn.Parameter(torch.zeros(1))

    def forward(self, x):
        batch_size, c, h, w = x.size()

        # Query, Key, Value projections
        query = self.query(x).view(batch_size, -1, h*w).permute(0, 2, 1)  # B x HW x C/8
        key = self.key(x).view(batch_size, -1, h*w)  # B x C/8 x HW
        value = self.value(x).view(batch_size, -1, h*w)  # B x C x HW

        # Attention map
        energy = torch.bmm(query, key)  # B x HW x HW
        attention = F.softmax(energy, dim=2)

        # Apply attention to value
        out = torch.bmm(value, attention.permute(0, 2, 1))  # B x C x HW
        out = out.view(batch_size, c, h, w)

        # Residual connection with learnable weight
        out = self.gamma * out + x

        return out


class ConvLSTMCell(nn.Module):
    """Convolutional LSTM cell for spatial-temporal modeling"""
    def __init__(self, input_dim, hidden_dim, kernel_size, bias=True):
        super(ConvLSTMCell, self).__init__()

        self.input_dim = input_dim
        self.hidden_dim = hidden_dim

        self.kernel_size = kernel_size
        self.padding = kernel_size // 2
        self.bias = bias

        # Gates: input, forget, output, cell
        self.conv = nn.Conv2d(
            in_channels=input_dim + hidden_dim,
            out_channels=4 * hidden_dim,
            kernel_size=kernel_size,
            padding=self.padding,
            bias=bias
        )

    def forward(self, input_tensor, cur_state):
        h_cur, c_cur = cur_state

        # Concatenate along channel axis
        combined = torch.cat([input_tensor, h_cur], dim=1)

        # Convolutional operation
        conv_output = self.conv(combined)

        # Split gates
        cc_i, cc_f, cc_o, cc_g = torch.split(conv_output, self.hidden_dim, dim=1)

        # Apply activations
        i = torch.sigmoid(cc_i)
        f = torch.sigmoid(cc_f)
        o = torch.sigmoid(cc_o)
        g = torch.tanh(cc_g)

        # Update states
        c_next = f * c_cur + i * g
        h_next = o * torch.tanh(c_next)

        return h_next, c_next

    def init_hidden(self, batch_size, image_size):
        height, width = image_size
        return (torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device),
                torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device))


class EncoderBlock(nn.Module):
    """Encoder block with residual connections"""
    def __init__(self, in_channels, out_channels, dropout_rate=0.2):
        super(EncoderBlock, self).__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(out_channels)
        self.relu = nn.LeakyReLU(0.2, inplace=True)
        self.dropout = nn.Dropout2d(dropout_rate)

        # Residual connection if dimensions don't match
        self.residual = nn.Conv2d(in_channels, out_channels, kernel_size=1) if in_channels != out_channels else nn.Identity()

    def forward(self, x):
        residual = self.residual(x)

        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)

        x = self.conv2(x)
        x = self.bn2(x)
        x = self.relu(x)

        x = x + residual  # Residual connection
        x = self.dropout(x)

        return x


class DecoderBlock(nn.Module):
    """Decoder block with skip connections"""
    def __init__(self, in_channels, mid_channels, out_channels, dropout_rate=0.2):
        super(DecoderBlock, self).__init__()
        self.conv1 = nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm2d(mid_channels)
        self.conv2 = nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(out_channels)
        self.relu = nn.LeakyReLU(0.2, inplace=True)
        self.dropout = nn.Dropout2d(dropout_rate)

    def forward(self, x, skip=None):
        if skip is not None:
            # Make sure skip connection has same spatial dimensions as x
            if x.shape[2:] != skip.shape[2:]:
                skip = F.interpolate(skip, size=x.shape[2:], mode='bilinear', align_corners=False)
            x = torch.cat([x, skip], dim=1)

        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)

        x = self.conv2(x)
        x = self.bn2(x)
        x = self.relu(x)

        x = self.dropout(x)

        return x

#----------------------------------------------------------------------------
# Enhanced ULSTM Model
#----------------------------------------------------------------------------

class EnhancedULSTM(nn.Module):
    """
    Enhanced ULSTM model with improved robustness and flexibility for varying input channels
    """
    def __init__(self, in_channels=5, out_channels=1, hidden_dim=64, lstm_kernel_size=3, time_steps=7,
                 use_attention=True, dropout_rate=0.2):
        super(EnhancedULSTM, self).__init__()

        self.in_channels = in_channels
        self.time_steps = time_steps
        self.hidden_dim = hidden_dim
        self.use_attention = use_attention

        # Ensure valid channel configurations
        if in_channels < 1:
            raise ValueError(f"Input channels must be at least 1, got {in_channels}")

        # Encoder blocks with automatic channel scaling
        self.encoder1 = EncoderBlock(in_channels, hidden_dim, dropout_rate)
        self.encoder2 = EncoderBlock(hidden_dim, hidden_dim*2, dropout_rate)
        self.encoder3 = EncoderBlock(hidden_dim*2, hidden_dim*4, dropout_rate)

        # Max pooling
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)

        # ConvLSTM for temporal modeling
        self.conv_lstm1 = ConvLSTMCell(hidden_dim, hidden_dim, lstm_kernel_size)
        self.conv_lstm2 = ConvLSTMCell(hidden_dim*2, hidden_dim*2, lstm_kernel_size)
        self.conv_lstm3 = ConvLSTMCell(hidden_dim*4, hidden_dim*4, lstm_kernel_size)

        # Attention blocks
        if use_attention:
            self.attention1 = AttentionBlock(hidden_dim)
            self.attention2 = AttentionBlock(hidden_dim*2)
            self.attention3 = AttentionBlock(hidden_dim*4)

        # Decoder blocks
        self.upconv2 = nn.ConvTranspose2d(hidden_dim*4, hidden_dim*2, kernel_size=2, stride=2)
        self.decoder2 = DecoderBlock(hidden_dim*4, hidden_dim*2, hidden_dim*2, dropout_rate)

        self.upconv1 = nn.ConvTranspose2d(hidden_dim*2, hidden_dim, kernel_size=2, stride=2)
        self.decoder1 = DecoderBlock(hidden_dim*2, hidden_dim, hidden_dim, dropout_rate)

        # Final output layer
        self.final_conv = nn.Conv2d(hidden_dim, out_channels, kernel_size=1)

    def forward(self, x):
        # Input shape: [batch_size, time_steps, channels, height, width]
        batch_size, seq_len, channels, height, width = x.size()

        # Initialize ConvLSTM hidden states with appropriate spatial dimensions
        h1, c1 = self.conv_lstm1.init_hidden(batch_size, (height//2, width//2))
        h2, c2 = self.conv_lstm2.init_hidden(batch_size, (height//4, width//4))
        h3, c3 = self.conv_lstm3.init_hidden(batch_size, (height//8, width//8))

        # Store encoder outputs for skip connections
        enc1_features = []
        enc2_features = []

        # Process each timestep through encoder
        for t in range(seq_len):
            x_t = x[:, t]

            # Encoder blocks with pooling
            enc1 = self.encoder1(x_t)
            enc1_features.append(enc1)
            x_t = self.pool(enc1)

            enc2 = self.encoder2(x_t)
            enc2_features.append(enc2)
            x_t = self.pool(enc2)

            enc3 = self.encoder3(x_t)
            x_t = self.pool(enc3)

            # Apply ConvLSTM to encoded features
            h1, c1 = self.conv_lstm1(self.pool(enc1), (h1, c1))
            h2, c2 = self.conv_lstm2(self.pool(enc2), (h2, c2))
            h3, c3 = self.conv_lstm3(x_t, (h3, c3))

        # Use the last ConvLSTM state for decoding
        x = h3

        # Apply attention if enabled
        if self.use_attention:
            x = self.attention3(x)

        # Decoder with skip connections from last timestep features
        x = self.upconv2(x)

        # Apply attention to skip connection if enabled
        if self.use_attention:
            enc2_att = self.attention2(enc2_features[-1])
            x = self.decoder2(x, enc2_att)
        else:
            x = self.decoder2(x, enc2_features[-1])

        # Continue decoding
        x = self.upconv1(x)

        # Apply attention to skip connection if enabled
        if self.use_attention:
            enc1_att = self.attention1(enc1_features[-1])
            x = self.decoder1(x, enc1_att)
        else:
            x = self.decoder1(x, enc1_features[-1])

        # Final output layer
        x = self.final_conv(x)

        # Ensure output matches input spatial dimensions
        if x.shape[2:] != (height, width):
            x = F.interpolate(x, size=(height, width), mode='bilinear', align_corners=False)

        return x

#----------------------------------------------------------------------------
# Functions for Chronological Data Split
#----------------------------------------------------------------------------

def create_chronological_data_loaders(tif_folder, batch_size=4, seq_length=7, input_channels=6,
                     fire_threshold=0.3, include_indices=True, val_ratio=0.2, test_ratio=0.1,
                     target_size=(112, 112), fire_band_idx=0, augment=False):
    """Create training, validation, and test data loaders with chronological splitting"""
    # Get all TIF files and sort chronologically
    tif_files = sorted([os.path.join(tif_folder, f) for f in os.listdir(tif_folder)
                         if f.endswith(".tif")])

    # Create the dataset but don't actually split yet
    full_dataset = RobustWildfireDataset(
        tif_folder=tif_folder,
        seq_length=seq_length,
        input_channels=input_channels,
        fire_threshold=fire_threshold,
        include_indices=include_indices,
        target_size=target_size,
        fire_band_idx=fire_band_idx,
        augment=augment
    )

    # Calculate dataset sizes for chronological split
    total_samples = len(full_dataset)
    train_end = int(total_samples * (1 - val_ratio - test_ratio))
    val_end = int(total_samples * (1 - test_ratio))

    # Create indices for the chronological split
    train_indices = list(range(0, train_end))
    val_indices = list(range(train_end, val_end))
    test_indices = list(range(val_end, total_samples))

    # Create subset datasets
    train_dataset = Subset(full_dataset, train_indices)
    val_dataset = Subset(full_dataset, val_indices)
    test_dataset = Subset(full_dataset, test_indices)

    print(f"Chronological split:")
    print(f"  Training: samples 0-{train_end-1} ({len(train_dataset)} samples)")
    print(f"  Validation: samples {train_end}-{val_end-1} ({len(val_dataset)} samples)")
    print(f"  Testing: samples {val_end}-{total_samples-1} ({len(test_dataset)} samples)")

    # Create data loaders
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,  # Can still shuffle within the training set
        pin_memory=True,
        num_workers=0,
        drop_last=False
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,  # Don't shuffle validation set
        pin_memory=True,
        num_workers=0,
        drop_last=False
    )

    test_loader = DataLoader(
        test_dataset,
        batch_size=batch_size,
        shuffle=False,  # Don't shuffle test set
        pin_memory=True,
        num_workers=0,
        drop_last=False
    )

    return train_loader, val_loader, test_loader, full_dataset

#----------------------------------------------------------------------------
# Training Function
#----------------------------------------------------------------------------

def train_wildfire_model(model, train_loader, val_loader, epochs=50, learning_rate=3e-4, device='cuda',
                        patience=10, save_dir='./wildfire_model', use_combined_loss=True, pos_weight=None,
                        scheduler_type='cosine', weight_decay=1e-5):
    """
    Enhanced training function with more options and better error handling
    """
    # Create save directory if it doesn't exist
    os.makedirs(save_dir, exist_ok=True)

    # Move model to device
    model = model.to(device)

    # Initialize optimizer with weight decay
    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)

    # Select learning rate scheduler
    if scheduler_type == 'cosine':
        scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
            optimizer, T_0=10, T_mult=2, eta_min=learning_rate / 100
        )
    elif scheduler_type == 'plateau':
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='min', factor=0.5, patience=5, verbose=True
        )
    elif scheduler_type == 'step':
        scheduler = optim.lr_scheduler.StepLR(
            optimizer, step_size=10, gamma=0.5
        )
    else:
        scheduler = None
        print(f"Unknown scheduler type: {scheduler_type}. Not using any scheduler.")

    # Initialize loss function
    if use_combined_loss:
        criterion = CombinedLoss(
            bce_weight=1.0,
            dice_weight=1.0,
            focal_weight=0.5,
            pos_weight=torch.tensor([pos_weight]).to(device) if pos_weight else None
        )
        print("Using combined loss (BCE + Dice + Focal)")
    else:
        criterion = nn.BCEWithLogitsLoss(
            pos_weight=torch.tensor([pos_weight]).to(device) if pos_weight else None
        )
        print("Using standard BCE loss")

    # Initialize variables for early stopping
    best_val_loss = float('inf')
    early_stop_counter = 0
    best_model = None

    # History dictionary for metrics
    history = {
        'train_loss': [],
        'val_loss': [],
        'val_f1': [],
        'val_precision': [],
        'val_recall': [],
        'learning_rate': []
    }

    # Training loop with error handling
    for epoch in range(epochs):
        try:
            # Training phase
            model.train()
            total_train_loss = 0.0
            train_batches = 0

            # Use tqdm for progress bar, catch and handle errors per batch
            train_iterator = tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs} [Train]")
            for batch_idx, (sequence, targets) in enumerate(train_iterator):
                try:
                    sequence, targets = sequence.to(device), targets.to(device)

                    # Zero gradients
                    optimizer.zero_grad()

                    # Forward pass
                    outputs = model(sequence)

                    # Compute loss
                    loss = criterion(outputs, targets)

                    # Backward pass and optimization
                    loss.backward()

                    # Gradient clipping
                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

                    # Update weights
                    optimizer.step()

                    # Track metrics
                    total_train_loss += loss.item()
                    train_batches += 1

                    # Update progress bar
                    train_iterator.set_postfix({'loss': loss.item()})

                except Exception as e:
                    print(f"Error in training batch {batch_idx}: {e}")
                    # Skip this batch and continue with the next one
                    continue

            # Calculate average training loss (handle empty case)
            avg_train_loss = total_train_loss / max(1, train_batches)

            # Validation phase
            model.eval()
            total_val_loss = 0.0
            val_preds = []
            val_targets = []
            val_batches = 0

            with torch.no_grad():
                val_iterator = tqdm(val_loader, desc=f"Epoch {epoch+1}/{epochs} [Val]")
                for batch_idx, (sequence, targets) in enumerate(val_iterator):
                    try:
                        sequence, targets = sequence.to(device), targets.to(device)

                        # Forward pass
                        outputs = model(sequence)

                        # Compute loss
                        loss = criterion(outputs, targets)

                        # Track metrics
                        total_val_loss += loss.item()
                        val_batches += 1

                        # Generate predictions for metrics
                        preds = (torch.sigmoid(outputs) > 0.5).float().cpu().numpy()
                        targets_np = targets.cpu().numpy()

                        val_preds.extend(preds.flatten())
                        val_targets.extend(targets_np.flatten())

                        # Update progress bar
                        val_iterator.set_postfix({'loss': loss.item()})

                    except Exception as e:
                        print(f"Error in validation batch {batch_idx}: {e}")
                        # Skip this batch and continue with the next one
                        continue

            # Calculate average validation loss (handle empty case)
            avg_val_loss = total_val_loss / max(1, val_batches)

            # Calculate validation metrics if there are positive examples
            if sum(val_targets) > 0:
                val_f1 = f1_score(val_targets, val_preds, average='weighted', zero_division=0)
                val_precision = precision_score(val_targets, val_preds, average='weighted', zero_division=0)
                val_recall = recall_score(val_targets, val_preds, average='weighted', zero_division=0)
            else:
                val_f1 = val_precision = val_recall = 0.0

            # Update learning rate
            current_lr = optimizer.param_groups[0]['lr']
            if scheduler_type == 'plateau':
                scheduler.step(avg_val_loss)
            elif scheduler is not None:
                scheduler.step()

            # Store metrics in history
            history['train_loss'].append(avg_train_loss)
            history['val_loss'].append(avg_val_loss)
            history['val_f1'].append(val_f1)
            history['val_precision'].append(val_precision)
            history['val_recall'].append(val_recall)
            history['learning_rate'].append(current_lr)

            # Print metrics
            print(f"Epoch {epoch+1}/{epochs}:")
            print(f"  Train Loss: {avg_train_loss:.4f}")
            print(f"  Val Loss: {avg_val_loss:.4f}, F1: {val_f1:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}")
            print(f"  LR: {current_lr:.6f}")

            # Check for improvement
            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                early_stop_counter = 0

                # Save best model
                best_model = model.state_dict().copy()
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'loss': avg_val_loss,
                    'f1': val_f1,
                    'precision': val_precision,
                    'recall': val_recall,
                    'history': history,
                    'model_config': {
                        'in_channels': model.in_channels,
                        'hidden_dim': model.hidden_dim,
                        'time_steps': model.time_steps,
                        'use_attention': model.use_attention
                    }
                }, f"{save_dir}/best_model.pt")

                print(f"  New best model saved with validation loss: {avg_val_loss:.4f}")
            else:
                early_stop_counter += 1
                print(f"  No improvement for {early_stop_counter} epochs")

            # Early stopping
            if early_stop_counter >= patience:
                print(f"Early stopping triggered after {epoch+1} epochs")
                break

            # Save checkpoint every 5 epochs for backup
            if (epoch + 1) % 5 == 0:
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'loss': avg_val_loss,
                    'history': history
                }, f"{save_dir}/checkpoint_epoch_{epoch+1}.pt")

        except Exception as e:
            print(f"Error in epoch {epoch+1}: {e}")
            # Save checkpoint before continuing to next epoch
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'history': history
            }, f"{save_dir}/error_checkpoint_epoch_{epoch+1}.pt")

            print(f"Checkpoint saved. Continuing to next epoch.")
            continue

    # Load best model
    if best_model is not None:
        model.load_state_dict(best_model)
        print(f"Loaded best model with validation loss: {best_val_loss:.4f}")

    # Also save final model
    torch.save({
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'history': history,
        'model_config': {
            'in_channels': model.in_channels,
            'hidden_dim': model.hidden_dim,
            'time_steps': model.time_steps,
            'use_attention': model.use_attention
        }
    }, f"{save_dir}/final_model.pt")

    # Plot training history
    plot_training_history(history, save_path=f"{save_dir}/training_history.png")

    return model, history

#----------------------------------------------------------------------------
# Evaluation Function
#----------------------------------------------------------------------------

def evaluate_model(model, data_loader, device, threshold=0.5):
    """Evaluate model on a dataset and return metrics"""
    model.eval()
    criterion = nn.BCEWithLogitsLoss()

    all_preds = []
    all_targets = []
    total_loss = 0.0
    num_batches = 0

    with torch.no_grad():
        for batch_idx, (sequence, targets) in enumerate(tqdm(data_loader, desc="Evaluating")):
            try:
                sequence, targets = sequence.to(device), targets.to(device)

                # Forward pass
                outputs = model(sequence)

                # Compute loss
                loss = criterion(outputs, targets)
                total_loss += loss.item()
                num_batches += 1

                # Generate predictions for metrics
                preds = (torch.sigmoid(outputs) > threshold).float().cpu().numpy()
                targets_np = targets.cpu().numpy()

                all_preds.extend(preds.flatten())
                all_targets.extend(targets_np.flatten())

            except Exception as e:
                print(f"Error in batch {batch_idx}: {e}")
                continue

    # Calculate metrics
    if sum(all_targets) > 0 and len(all_targets) > 0:
        precision = precision_score(all_targets, all_preds, zero_division=0)
        recall = recall_score(all_targets, all_preds, zero_division=0)
        f1 = f1_score(all_targets, all_preds, zero_division=0)
        cm = confusion_matrix(all_targets, all_preds)
    else:
        precision = recall = f1 = 0.0
        cm = np.zeros((2, 2))

    # Calculate average loss
    avg_loss = total_loss / max(1, num_batches)

    return {
        'loss': avg_loss,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'confusion_matrix': cm,
        'predictions': all_preds,
        'targets': all_targets
    }

#----------------------------------------------------------------------------
# Visualization Functions
#----------------------------------------------------------------------------

def plot_training_history(history, save_path=None):
    """Plot training metrics"""
    epochs = range(1, len(history['train_loss']) + 1)

    # Create figure with 2x2 subplots
    fig, axs = plt.subplots(2, 2, figsize=(15, 10))

    # Plot loss
    axs[0, 0].plot(epochs, history['train_loss'], 'b-', label='Training Loss')
    axs[0, 0].plot(epochs, history['val_loss'], 'r-', label='Validation Loss')
    axs[0, 0].set_title('Loss')
    axs[0, 0].set_xlabel('Epochs')
    axs[0, 0].set_ylabel('Loss')
    axs[0, 0].legend()
    axs[0, 0].grid(True)

    # Plot F1 score
    axs[0, 1].plot(epochs, history['val_f1'], 'g-', label='Validation F1')
    axs[0, 1].set_title('F1 Score')
    axs[0, 1].set_xlabel('Epochs')
    axs[0, 1].set_ylabel('F1 Score')
    axs[0, 1].legend()
    axs[0, 1].grid(True)

    # Plot precision and recall
    axs[1, 0].plot(epochs, history['val_precision'], 'g-', label='Validation Precision')
    axs[1, 0].plot(epochs, history['val_recall'], 'orange', label='Validation Recall')
    axs[1, 0].set_title('Precision and Recall')
    axs[1, 0].set_xlabel('Epochs')
    axs[1, 0].set_ylabel('Score')
    axs[1, 0].legend()
    axs[1, 0].grid(True)

    # Plot learning rate
    axs[1, 1].plot(epochs, history['learning_rate'], 'purple', label='Learning Rate')
    axs[1, 1].set_title('Learning Rate')
    axs[1, 1].set_xlabel('Epochs')
    axs[1, 1].set_ylabel('Learning Rate')
    axs[1, 1].set_yscale('log')  # Use log scale for learning rate
    axs[1, 1].grid(True)

    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, dpi=200)
        print(f"Training history plot saved to {save_path}")

    plt.show()


def visualize_predictions(model, dataset, indices=None, num_samples=5,
                          threshold=0.5, save_dir="./visualizations",
                          device=None, random_samples=False):
    """
    Robust visualization function for wildfire predictions with error handling
    """
    # Create output directory
    os.makedirs(save_dir, exist_ok=True)

    # If device not specified, use the device of the model
    if device is None:
        device = next(model.parameters()).device

    # Set model to evaluation mode
    model.eval()

    # Select indices to visualize
    if indices is None:
        if random_samples:
            indices = torch.randperm(len(dataset))[:num_samples].tolist()
        else:
            indices = list(range(min(num_samples, len(dataset))))

    # Metrics for selected samples
    all_preds = []
    all_targets = []

    # Create a summary figure for all samples
    summary_fig, summary_axs = plt.subplots(len(indices), 3, figsize=(15, 5*len(indices)))
    if len(indices) == 1:
        summary_axs = [summary_axs]  # Handle single row case

    # Process each sample with error handling
    for i, idx in enumerate(indices):
        try:
            # Get sample
            sequence, target = dataset[idx]
            sequence = sequence.unsqueeze(0).to(device)  # Add batch dimension

            # Get prediction with probability map
            with torch.no_grad():
                output = model(sequence)
                prob_map = torch.sigmoid(output).cpu().squeeze().numpy()
                pred_binary = (prob_map > threshold).astype(np.float32)

            # Move tensors to CPU for visualization
            # Use first three channels for RGB visualization if available
            if sequence.shape[2] >= 3:
                input_image = sequence[0, -1, 0:3].cpu().permute(1, 2, 0).numpy()
                input_image = np.clip(input_image, 0, 1)  # Ensure valid RGB range
            else:
                # Use just the first channel if there are fewer than 3 channels
                input_image = sequence[0, -1, 0:1].cpu().repeat(1, 1, 3).permute(1, 2, 0).numpy()
                input_image = np.clip(input_image, 0, 1)

            target = target.cpu().squeeze().numpy()

            # Collect metrics
            all_preds.append(pred_binary.flatten())
            all_targets.append(target.flatten())

            # Create detailed figure for this sample
            fig, axs = plt.subplots(2, 2, figsize=(12, 10))

            # Plot input image
            axs[0, 0].imshow(input_image)
            axs[0, 0].set_title('Input Image (Last Frame)')
            axs[0, 0].axis('off')

            # Plot ground truth
            axs[0, 1].imshow(target, cmap='hot')
            axs[0, 1].set_title('Ground Truth Fire Mask')
            axs[0, 1].axis('off')

            # Plot probability map
            im = axs[1, 0].imshow(prob_map, cmap='plasma', vmin=0, vmax=1)
            axs[1, 0].set_title(f'Fire Probability Map')
            axs[1, 0].axis('off')
            plt.colorbar(im, ax=axs[1, 0], fraction=0.046, pad=0.04)

            # Plot binary prediction
            axs[1, 1].imshow(pred_binary, cmap='hot')
            axs[1, 1].set_title(f'Predicted Fire Mask (t={threshold})')
            axs[1, 1].axis('off')

            # Calculate metrics for this sample
            precision = precision_score(target.flatten(), pred_binary.flatten(), zero_division=0)
            recall = recall_score(target.flatten(), pred_binary.flatten(), zero_division=0)
            f1 = f1_score(target.flatten(), pred_binary.flatten(), zero_division=0)

            # Add metrics as text
            fig.text(0.5, 0.01, f'Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}',
                    ha='center', fontsize=12)

            plt.tight_layout()
            plt.savefig(f"{save_dir}/sample_{idx}_detailed.png", dpi=200)
            plt.close(fig)

            # Create overlay visualization
            overlay_fig = plt.figure(figsize=(8, 8))
            plt.imshow(input_image)

            # Add fire prediction as semi-transparent overlay
            fire_mask = np.zeros((*pred_binary.shape, 4))  # RGBA
            fire_mask[..., 0] = 1.0  # Red channel
            fire_mask[..., 3] = pred_binary * 0.7  # Alpha channel

            plt.imshow(fire_mask)
            plt.title(f'Sample {idx}: Fire Prediction Overlay')
            plt.axis('off')
            plt.tight_layout()
            plt.savefig(f"{save_dir}/sample_{idx}_overlay.png", dpi=200)
            plt.close(overlay_fig)

            # Add to summary plot
            summary_axs[i, 0].imshow(input_image)
            summary_axs[i, 0].set_title(f'Sample {idx}: Input')
            summary_axs[i, 0].axis('off')

            # Create a difference image (True Positive, False Positive, False Negative)
            diff_img = np.zeros((*pred_binary.shape, 3))
            diff_img[..., 0] = np.logical_and(pred_binary == 0, target == 1)  # False Negative (Red)
            diff_img[..., 1] = np.logical_and(pred_binary == 1, target == 1)  # True Positive (Green)
            diff_img[..., 2] = np.logical_and(pred_binary == 1, target == 0)  # False Positive (Blue)

            summary_axs[i, 1].imshow(target, cmap='hot')
            summary_axs[i, 1].set_title('Ground Truth')
            summary_axs[i, 1].axis('off')

            summary_axs[i, 2].imshow(diff_img)
            summary_axs[i, 2].set_title(f'Comparison (F1: {f1:.2f})')
            summary_axs[i, 2].axis('off')

            print(f"Processed sample {idx} (Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f})")

        except Exception as e:
            print(f"Error processing sample {idx}: {e}")
            # Skip this sample
            continue

    # Finalize and save summary figure
    plt.tight_layout()
    summary_fig.savefig(f"{save_dir}/prediction_summary.png", dpi=200)
    plt.close(summary_fig)

    # Check if we have predictions and targets
    if not all_preds or not all_targets:
        print("No valid samples processed. Cannot calculate overall metrics.")
        return None

    # Calculate overall metrics
    try:
        all_preds_flat = np.concatenate([p for p in all_preds])
        all_targets_flat = np.concatenate([t for t in all_targets])

        overall_precision = precision_score(all_targets_flat, all_preds_flat, zero_division=0)
        overall_recall = recall_score(all_targets_flat, all_preds_flat, zero_division=0)
        overall_f1 = f1_score(all_targets_flat, all_preds_flat, zero_division=0)

        # Create confusion matrix
        cm = confusion_matrix(all_targets_flat, all_preds_flat)
        cm_fig, cm_ax = plt.subplots(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=cm_ax)
        cm_ax.set_xlabel('Predicted')
        cm_ax.set_ylabel('Actual')
        cm_ax.set_title('Confusion Matrix')
        plt.tight_layout()
        plt.savefig(f"{save_dir}/confusion_matrix.png", dpi=200)
        plt.close(cm_fig)

        # Create summary metrics file
        with open(f"{save_dir}/metrics_summary.txt", 'w') as f:
            f.write(f"Overall Metrics for {len(indices)} Samples:\n")
            f.write(f"Precision: {overall_precision:.4f}\n")
            f.write(f"Recall: {overall_recall:.4f}\n")
            f.write(f"F1 Score: {overall_f1:.4f}\n\n")
            f.write("Confusion Matrix:\n")
            f.write(f"True Negative: {cm[0, 0]}\n")
            f.write(f"False Positive: {cm[0, 1]}\n")
            f.write(f"False Negative: {cm[1, 0]}\n")
            f.write(f"True Positive: {cm[1, 1]}\n")

        print(f"\nVisualization complete! Results saved to {save_dir}")
        print(f"Overall Metrics - Precision: {overall_precision:.4f}, Recall: {overall_recall:.4f}, F1: {overall_f1:.4f}")

        return {
            'precision': overall_precision,
            'recall': overall_recall,
            'f1': overall_f1,
            'confusion_matrix': cm,
            'samples': indices
        }

    except Exception as e:
        print(f"Error calculating overall metrics: {e}")
        return None

#----------------------------------------------------------------------------
# Main Execution Function
#----------------------------------------------------------------------------
def main():
    # Define parameters with improved defaults
    params = {
        'tif_folder': "/content/drive/My Drive/wildfire_project/LSTM_Data",
        'batch_size': 4,
        'seq_length': 7,
        'input_channels': 6,  # This will be adjusted based on actual data
        'fire_threshold': 0.1,
        'include_indices': True,
        'val_ratio': 0.15,
        'test_ratio': 0.15,  # Added test ratio for chronological split
        'target_size': (112, 112),
        'fire_band_idx': 0,  # Using a safer default (band 0)
        'epochs': 10,
        'learning_rate': 3e-4,
        'patience': 5,
        'save_dir': "/content/drive/MyDrive/wildfire_model_chronological",
        'use_attention': False,
        'use_combined_loss': False,
        'augment': False,
        'scheduler_type': 'cosine',
        'weight_decay': 1e-5,
        'dropout_rate': 0.3,  # Increased dropout for better generalization
    }

    # Try to mount Google Drive if in Colab
    try:
        from google.colab import drive
        if not os.path.exists('/content/drive/MyDrive'):
            drive.mount('/content/drive')
            print("Google Drive mounted successfully")
    except:
        print("Not running in Google Colab or drive already mounted.")

    # Check if the TIF folder exists, otherwise suggest alternatives
    tif_folder_options = [
        params['tif_folder'],
        params['tif_folder'].replace('My Drive', 'MyDrive'),
        params['tif_folder'].replace('MyDrive', 'My Drive'),
        os.path.join('/content/drive/MyDrive', os.path.basename(params['tif_folder'])),
        os.path.join('/content/drive/My Drive', os.path.basename(params['tif_folder'])),
        '/content/sample_data'  # fallback to sample data directory
    ]

    # Find the first path that exists or create sample data
    found_tif_folder = False
    for path in tif_folder_options:
        if os.path.exists(path):
            files = [f for f in os.listdir(path) if f.endswith('.tif')]
            if files:
                params['tif_folder'] = path
                found_tif_folder = True
                print(f"Found {len(files)} TIF files in {path}")
                break

    # If no TIF directory found, create sample data
    if not found_tif_folder:
        print("No TIF directory found. Creating sample data...")
        import numpy as np

        # Create sample directory
        sample_dir = '/content/sample_data'
        os.makedirs(sample_dir, exist_ok=True)

        # Create 30 dummy TIF files with varying channels (5-7) to test robustness
        for i in range(30):
            # Randomly choose between 5-7 channels to simulate real-world variance
            num_channels = np.random.randint(5, 8)
            # Create a random 100x100 array with variable channels
            array = np.random.rand(num_channels, 100, 100).astype(np.float32)

            # Add some random fire patterns to make the data more interesting
            if i > 20:  # Add fire patterns to later files to simulate time progression
                fire_mask = np.zeros((1, 100, 100), dtype=np.float32)
                # Create a random fire circle
                cx, cy = np.random.randint(30, 70, size=2)
                radius = np.random.randint(5, 15)
                y, x = np.ogrid[:100, :100]
                fire_mask[0, ((x - cx) ** 2 + (y - cy) ** 2) <= radius ** 2] = 1.0

                # Add the fire mask as the first channel
                array[0] = fire_mask[0]

            # Save as TIF file
            with rasterio.open(
                os.path.join(sample_dir, f'sample_{i+1:02d}.tif'),
                'w',
                driver='GTiff',
                height=array.shape[1],
                width=array.shape[2],
                count=array.shape[0],
                dtype=array.dtype
            ) as dst:
                dst.write(array)

        params['tif_folder'] = sample_dir
        print(f"Created sample data directory with 30 TIF files at: {sample_dir}")

    print("\n-----------------------------------------------------")
    print("Creating data loaders with chronological split...")
    print("-----------------------------------------------------\n")

    train_loader, val_loader, test_loader, full_dataset = create_chronological_data_loaders(
        tif_folder=params['tif_folder'],
        batch_size=params['batch_size'],
        seq_length=params['seq_length'],
        input_channels=params['input_channels'],
        fire_threshold=params['fire_threshold'],
        include_indices=params['include_indices'],
        val_ratio=params['val_ratio'],
        test_ratio=params['test_ratio'],
        target_size=params['target_size'],
        fire_band_idx=params['fire_band_idx'],
        augment=params['augment']
    )

    print("\n-----------------------------------------------------")
    print("Creating model...")
    print("-----------------------------------------------------\n")

    # Use the actual channel count from the dataset
    input_channels = full_dataset.input_channels
    model = EnhancedULSTM(
        in_channels=input_channels,
        out_channels=1,
        hidden_dim=64,
        lstm_kernel_size=3,
        time_steps=params['seq_length'],
        use_attention=params['use_attention'],
        dropout_rate=params['dropout_rate']
    )

    # Calculate class weights for imbalanced dataset
    pos_weight = None
    if params['use_combined_loss']:
        print("Calculating positive class weight...")
        try:
            # Sample a subset of the dataset to calculate class weights
            num_samples = min(1000, len(full_dataset))
            indices = torch.randperm(len(full_dataset))[:num_samples]
            fire_pixels, total_pixels = 0, 0

            for idx in indices:
                try:
                    _, target = full_dataset[idx]
                    fire_pixels += torch.sum(target).item()
                    total_pixels += target.numel()
                except Exception as e:
                    print(f"Error processing sample {idx} for class weight: {e}")
                    continue

            if total_pixels > 0:
                fire_ratio = fire_pixels / total_pixels
                print(f"Fire pixel ratio in sample: {fire_ratio:.4f}")

                # Calculate weight based on ratio
                if fire_ratio > 0:
                    pos_weight = (1 - fire_ratio) / fire_ratio
                    pos_weight = min(pos_weight, 50.0)  # Cap the weight to prevent training instability
                    print(f"Using positive class weight: {pos_weight:.2f}")
                else:
                    print("No fire pixels found in sample. Using default weight.")
                    pos_weight = 10.0  # Default weight if no fire pixels found
            else:
                print("No valid pixels found for class weight calculation. Using default weight.")
                pos_weight = 10.0
        except Exception as e:
            print(f"Error calculating class weights: {e}")
            pos_weight = 10.0  # Default weight if calculation fails

    print("\n-----------------------------------------------------")
    print("Starting training...")
    print("-----------------------------------------------------\n")

    model, history = train_wildfire_model(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        epochs=params['epochs'],
        learning_rate=params['learning_rate'],
        device=device,
        patience=params['patience'],
        save_dir=params['save_dir'],
        use_combined_loss=params['use_combined_loss'],
        pos_weight=pos_weight,
        scheduler_type=params['scheduler_type'],
        weight_decay=params['weight_decay']
    )

    # Create visualizations directory
    vis_dir = f"{params['save_dir']}/visualizations"
    os.makedirs(vis_dir, exist_ok=True)

    print("\n-----------------------------------------------------")
    print("Generating visualizations of validation set...")
    print("-----------------------------------------------------\n")

    # Get some validation indices for visualization
    val_indices = list(range(len(full_dataset) - len(test_loader.dataset), len(full_dataset)))[:5]

    visualization_results = visualize_predictions(
        model=model,
        dataset=full_dataset,
        indices=val_indices,
        threshold=0.5,
        save_dir=vis_dir,
        device=device
    )

    # Print visualization summary if available
    if visualization_results:
        print("\nVisualization Summary:")
        print(f"Overall Precision: {visualization_results['precision']:.4f}")
        print(f"Overall Recall: {visualization_results['recall']:.4f}")
        print(f"Overall F1 Score: {visualization_results['f1']:.4f}")

        # Plot confusion matrix
        cm = visualization_results['confusion_matrix']
        print("\nConfusion Matrix:")
        print(f"True Negative: {cm[0, 0]}")
        print(f"False Positive: {cm[0, 1]}")
        print(f"False Negative: {cm[1, 0]}")
        print(f"True Positive: {cm[1, 1]}")

    print("\n-----------------------------------------------------")
    print("Evaluating on chronological test set...")
    print("-----------------------------------------------------\n")

    # Evaluate on the test set
    test_metrics = evaluate_model(model, test_loader, device, threshold=0.5)

    # Print test metrics
    print("\nTest Set Evaluation:")
    print(f"Test Loss: {test_metrics['loss']:.4f}")
    print(f"Test F1 Score: {test_metrics['f1']:.4f}")
    print(f"Test Precision: {test_metrics['precision']:.4f}")
    print(f"Test Recall: {test_metrics['recall']:.4f}")

    # Save test metrics
    with open(f"{params['save_dir']}/test_metrics.txt", 'w') as f:
        f.write(f"Test Set Evaluation:\n")
        f.write(f"Loss: {test_metrics['loss']:.4f}\n")
        f.write(f"F1 Score: {test_metrics['f1']:.4f}\n")
        f.write(f"Precision: {test_metrics['precision']:.4f}\n")
        f.write(f"Recall: {test_metrics['recall']:.4f}\n")
        f.write(f"Confusion Matrix:\n")
        f.write(f"True Negative: {test_metrics['confusion_matrix'][0, 0]}\n")
        f.write(f"False Positive: {test_metrics['confusion_matrix'][0, 1]}\n")
        f.write(f"False Negative: {test_metrics['confusion_matrix'][1, 0]}\n")
        f.write(f"True Positive: {test_metrics['confusion_matrix'][1, 1]}\n")

    print(f"\nAll results saved to {params['save_dir']}")
    print("Training and evaluation complete!")

    # Compare random split vs. chronological split performance if available
    try:
        random_metrics_path = params['save_dir'].replace('_chronological', '')
        random_metrics_file = f"{random_metrics_path}/test_metrics.txt"
        if os.path.exists(random_metrics_file):
            print("\n-----------------------------------------------------")
            print("Comparison between Random and Chronological Split:")
            print("-----------------------------------------------------\n")

            # Read random split metrics
            with open(random_metrics_file, 'r') as f:
                random_metrics = f.read()

            print("Random Split Metrics:")
            print(random_metrics)

            print("\nChronological Split Metrics:")
            print(f"Loss: {test_metrics['loss']:.4f}")
            print(f"F1 Score: {test_metrics['f1']:.4f}")
            print(f"Precision: {test_metrics['precision']:.4f}")
            print(f"Recall: {test_metrics['recall']:.4f}")

            # Save comparison
            with open(f"{params['save_dir']}/split_comparison.txt", 'w') as f:
                f.write("Comparison between Random and Chronological Split:\n\n")
                f.write("Random Split Metrics:\n")
                f.write(random_metrics)
                f.write("\nChronological Split Metrics:\n")
                f.write(f"Loss: {test_metrics['loss']:.4f}\n")
                f.write(f"F1 Score: {test_metrics['f1']:.4f}\n")
                f.write(f"Precision: {test_metrics['precision']:.4f}\n")
                f.write(f"Recall: {test_metrics['recall']:.4f}\n")
    except Exception as e:
        print(f"Could not compare split methods: {e}")

    return model, full_dataset, train_loader, val_loader, test_loader, history


if __name__ == "__main__":
    main()
